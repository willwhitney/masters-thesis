# Disentanglement in Computation

## Introduction

While the learning of representation in fields such as vision and language have been extensively studied, representations in computation have only recently begun to be studied.

Instead of thinking about _data_, like images or text, representations of computation are about representing _procedures_, computation itself. Instead of representing an image, we might represent a transformation of that image, like rotating every object in it by 90Â°. Instead of representing the words in an English phrase, we might represent a program which translates it into French.

Every neural network can be thought of as a representation of a computation. The weights and nonlinearities in the networks combine to transform some input data to some output data; in action they are a _function_ from an input domain to an output domain, and in storage they represent this function. The famous ImageNet network by Krizhevsky et al. [-@krizhevsky2012imagenet], for example, contains 60 million parameters which, along with their connectivity, define a function from an input domain of 256x256x3 image to a 1x1000 distribution over labels.

Applying our desiderata for representations, let us consider the quality of this 60-million-weight representation for a function which classifies images.

1. **Disentangled**: Just as any representation of data should be sparse over real transformations, the representation of the transformation itself should be sparse. The only clear factorization of the computation represented by a feedforward neural network is the factorization into layers. Each layer of the network represents a large matrix multiplication, and the function computed is the same for all inputs. This representation for computation is not at all sparse over its inputs, for the entire computation is performed no matter what the input is.

1. **Interpretable**: Neural networks are famously hard to interpret. Researchers have developed whole classes of techniques for analyzing them, which use gradient ascent to visualize specific units of the network [@erhan2009visualizing], occlusions of the input to analyze significance [@zeiler2014visualizing], or inverting their functions to visualize their information preservation [@mahendran2014understanding]. These techniques speak to the deeply uninterpretable nature of neural representations of computation.

1. **Performant**: Deep neural networks currently hold the accuracy records in almost every large-N dataset of image recognition, object localization, and phoneme recognition benchmark. In particular, this network set the record for ImageNet performance with an error rate more than 40% lower than any other entry.

1. **Reusable**: While substantial reuse of pretrained copies of this network has been made, such reuse is by no means simple. Typically the top half of the network is completely removed and another is trained (quite expensively) in its place; in other use cases the lower-level features generated by the first few layers of the network have been used directly as an embedding of the input space, with very mixed results. Compared with a more modular design, which might have separate components for localizing salient objects and determining various salient information about them (size, color, animacy, shape, context) this representation is quite hard to reuse.

1. **Compact**: This model contains 60 million parameters. It occupies hundreds of megabytes on disk when compressed. While those numbers sound large, it is not immediately clear if this is very large or very compact for a model which contains all necessary information for determining the contents of arbitrary images.

While this model performs extremely well, and might (in bad lighting, with the right Instagram filter) be considered compact, it is very far from ideal in disentanglement, interpretability, and reusability. Just by disentangling the computation in this model, factorizing it into modules, its interpretability and reusability would be hugely improved.

If, for example, this network were disentangled by having a module which determined whether a scene was indoors or outdoors and a separate classifier for each of those cases, we would gain several advantages:

- The indoor/outdoor classifier would be immediately comprehensible.
- The indoor/outdoor classifier could be reused in other tasks.
- The location-specific object classifiers could be more easily interpreted (e.g. you would be very surprised if the indoor classifier predicted a train, or a gorilla).
- The location-specific object classifiers would generate intermediate features which were more diagnostic for other tasks in their given location.

To make an unfair comparison, let's use our desiderata to consider the quality of a Python representation of the computation of the FizzBuzz problem:

```{#lst:fizzbuzz .python}
def divisible_by_five(n):
	return n % 5 == 0

def divisible_by_three(n):
	return n % 3 == 0

def fizzbuzz(n):
	result = ''
	if divisible_by_three(n):
		result += 'fizz'
	if divisible_by_five(n):
		result += 'buzz'
	return result

def fizzbuzz_string(length):
	result_list = map(fizzbuzz, range(1, length + 1))
	return '\n'.join(result_list)

print(fizzbuzz_string(100))
```


1. **Disentangled**: This computation has been factorized into a number of distinct subcomponents, each of which is very small and can be used in multiple places. They do not depend on the state of the overall program, and have very low-dimensional and clearly-defined inputs and outputs. Simple operators such as `+=` or `%` are composed into larger ones, and the contribution from each is very clear.

1. **Interpretable**: This representation can be easily read by anyone who knows how to program, and most of it could be understood even by people who don't.

1. **Performant**: While this code will make no errors on the task, this is not a meaningful question on a toy task.

1. **Reusable**: Individual components of this code represent functions which could be used elsewhere or for variants of this task. It would be trivial to use `divisible_by_three` anywhere else its functionality is needed, and the other functions can similarly be reused to generate FizzBuzz solutions of any length.

1. **Compact**: This representation occupies 370 bytes.

Our ideal representation of a computation would share the learnability and performance on hard problems of the deep network without giving up the goals of disentanglement and reuse quite so completely as the deep network does.


### Catastrophic forgetting

One of the clearest demonstrations of the weakness of highly-entangled neural network representations of computation is catastrophic forgetting.

With the recent success of deep learning methods in many fields, efforts have been made to apply deep learning techniques to multitask learning problems. Deep learning is at the deepest level a method for hierarchically extracting good representations from complex data, with the higher levels of a network capturing increasingly abstract representations of the data. As such, deep learning seems naively to be a promising direction for multitask learning; abstract representations of the data should be useful for many related tasks, and the network should be able to simply not use any which are not helpful.

This theory has been borne out for simple, highly coupled tasks such as evaluating sentiment of reviews for different categories of products [@glorot2011domain]. A more wide-ranging survey of deep learning methods for transfer and multitask learning shows that some classes of models are able to improve their performance on the original, clean dataset after being shown perturbed or distorted versions of the same data [@bengio2012deep].

However, even small changes in the task result in substantial changes to the optimal features, especially at high levels of the network [@yosinski2014transferable]. This can lead to _catastrophic forgetting_, in which the network "unlearns" one task as it trains on another one. A recent set of experiments [@goodfellow2013empirical] detail the tradeoff curve for performance on one task versus performance on the other task for both similar and dissimilar tasks. They show that for networks trained on two tasks, improvement on one task comes at a cost to performance on another.

This occurs because of the highly entangled nature of the computation carried out by these networks. When the network which is able to solve two different tasks is retrained on just one, it gradually mutates the calculations which are necessary in both of the tasks, until eventually it has repurposed them entirely for the use of the first task. What this system needs is a clear separation of concerns. If some functional elements of the network were used only for one task, those elements would be safe to mutate at a high rate during training on that task. Similarly, those elements which were used across many tasks could change only very gradually, ensuring that even if one task is neglected for an extended period, the components it uses won't have diverged too greatly from their original state.

In more specific terms, the problem is that each weight in the network receives gradients of a similar magnitude when training on either task. And with no parameters "reserved" for a specific task, that task is quickly forgotten.
