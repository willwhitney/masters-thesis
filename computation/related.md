## Related Work

Until recently, work in this domain has largely centered around either a) learning programs in a fixed representation language, or b) jointly learning a program and its representation in e.g. a neural network, but with little attention focus on the representation itself. In particular, Liang et al. [@liang2010learning] propose to learn programs via Bayesian inference with a grammar over a hierarchical structure. Zaremba et al. [-@zaremba2014learning] use an LSTM [@hochreiter1997long] to predict the output of simple programs written in Python; their effectiveness is remarkable, but the induced representation is so poor that the authors comment, "We do not know how heavily our model relies on memorization and how far the learned algorithm is from the actual, correct algorithm."

A classic model that attempts to disentangle computation is the mixture of experts [@jacobs1991task]. However, as originally described this model was not especially successful at learning distinct functions for each expert; this led to a modification of the design which used sampling instead of weighting using the gating values [@jacobs1991adaptive]. This modified design resulted in nicely decoupled functions, but was much harder to train. Addressing this problem was a core inspiration for my work.

In the last year, work on learning structured representations of computation has become a popular topic. [@neelakantan2015neural] augment a neural network with a small set of hard-coded external operations which the network learns to use in multistep programs. [@reed2015neural] propose a very general model which similarly can use external programs, but with the addition of a call stack; however, this model requires strong supervision to train explicitly with the correct program trace, and as such is learning to recreate an existing program representation. [@zaremba2015learning] use an external memory with pointers to learn routines for interacting with external data. [@graves2014neural] perform complex operations on sequences such as sorting or repeatedly copying by using a differentiable content-based addressing mechanism to read and write to an external memory.
