## Related Work

As mentioned previously, a number of generative models have been proposed in the literature to obtain abstract visual representations. Unlike most RBM-based models [@hinton2006fast, @salakhutdinov2009deep, @lee2009convolutional], our approach is trained using back-propagation with objective function consisting of data reconstruction and the variational bound.

Relatively recently, Kingma et al. [@kingma2013auto] proposed the SGVB algorithm to learn generative models with continuous latent variables. In this work, a feed-forward neural network (encoder) is used to approximate the posterior distribution and a decoder network serves to enable stochastic reconstruction of observations. In order to handle fine-grained geometry of faces, we work with relatively large scale images ($150 \times 150$ pixels). Our approach extends and applies the SGVB algorithm to jointly train and utilize many layers of convolution and de-convolution operators for the encoder and decoder network respectively. The decoder network is a function that transform a compact graphics code (200 dimensions) to a $150 \times 150$ image. We propose using unpooling (nearest neighbor sampling) followed by convolution to handle the massive increase in dimensionality with a manageable number of parameters.

[@dosovitskiy2015learning] proposed using CNNs to generate images given object-specific parameters in a supervised setting. As their approach requires ground-truth labels for the graphics code layer, it cannot be directly applied to image interpretation tasks. Our work is similar to Ranzato et al. [-@ranzato2007unsupervised], whose work was amongst the first to use a generic encoder-decoder architecture for feature learning. However, in comparison to our proposal their model was trained layer-wise, the intermediate representations were not disentangled like a graphics code, and their approach does not use the variational auto-encoder loss to approximate the posterior distribution. Our work is also similar in spirit to [@tang2012deep], but in comparison our model does not assume a Lambertian reflectance model and implicitly constructs the 3D representations. Another piece of related work is Desjardins et al. [-@desjardins2012disentangling], who used a spike and slab prior to factorize representations in a generative deep network.

Quite recently, [@jaderberg2015spatial] proposes a model which explicitly captures the pose of objects in a scene through the use of predefined 2D affine transformations, which leads pose and identity to be disentangled. [@mansimov2015generating] use an attention mechanism to generate images from text; this approach has the potential to learn functions which are parametrized by highly disentangled symbolic representations. [@theis2015generative] use spatial LSTMs to build generative models of textures in natural images.

In comparison to prior approaches, it is important to note that our encoder network produces the interpretable and disentangled representations necessary to learn a meaningful 3D graphics engine. A number of inverse-graphics inspired methods have recently been proposed in the literature [@mansinghka2013approximate]. However, most such methods rely on hand-crafted rendering engines. The exception to this is work by Hinton et al. [-@hinton2011transforming] and Tieleman [-@tieleman2014optimizing] on _transforming autoencoders_ which use a domain-specific decoder to reconstruct input images.

[@yang2015weakly] follows up on our work with a recurrent model which learns similar disentangled representations from watching synthesized video.
