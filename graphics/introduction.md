# Disentanglement in Vision

## Introduction

Deep learning has led to remarkable breakthroughs in learning hierarchical representations from images. Models such as Convolutional Neural Networks (CNNs) [@lecun1995convolutional], Restricted Boltzmann Machines, [@hinton2006fast, @salakhutdinov2009deep], and Auto-encoders [@bengio2009learning, @vincent2010stacked] have been successfully applied to produce multiple layers of increasingly abstract visual representations. However, there is relatively little work on characterizing the optimal representation of the data. While  Cohen et al. [-@cohen2014learning] have considered this problem by proposing a theoretical framework to learn irreducible representations with both invariances and equivariances, coming up with the best representation for any given task is an open question.

To shed some light on this question, let us consider our list of desires for representations in the specific context of vision.

1. **Disentangled**: When applied to real-world transformations over images, i.e. video, a good representation will change only sparsely. That is, the expectation over a set of videos of the number of dimensions of the representation which change between each frame should be small. In practice this means that common transformations, like movement of an object, should be expressed concisely; whereas uncommon transformations, like a solid object turning inside out, may be more expensive to express.

1. **Interpretable**: To be highly interpretable, a representation needs to line up well with the one that's in our heads. It should express objects separately from their conditions, and common transformations should be monotonic and smooth in representation space.

1. **Performant**: Representations of visual content need to be quite rich; in order to be able to solve problems like "Which of these objects is in front of the other?" a representation must understand much more than just pixels.

1. **Reusable**: A representation which was learned to solve a very specific task, like perhaps that of the DQN [@mnih2015human], will not be helpful in other settings, like the real world. To be reusable a model needs to capture the structure that is universal to our world.

1. **Compact**: Representations of images should be able to efficiently compress the images in their domain.

The "vision as inverse graphics" paradigm suggests a representation for images which provides these features. Computer graphics consists of a function to go from compact descriptions of scenes (the _graphics code_) to images, and this graphics code is typically disentangled to allow for rendering scenes with fine-grained control over transformations such as object location, pose, lighting, texture, and shape. This encoding is designed to easily and interpretably represent sequences of real data so that common transformations may be compactly represented in software code; this criterion is conceptually identical to disentanglement, and graphics codes conveniently align with the properties of an ideal representation. Graphics codes are the representations which we as a society have designed to be the single most general-purpose, interpretable, and generally usable way to express scenes.

Early work by Tenenbaum et al. [-@tenenbaum2000separating] was among the first to explore this idea, and used bilinear models to differentiate between extrinsic and intrinsic factors in images. Recent work in inverse graphics [@mansinghka2013approximate, @kulkarni2014inverse, @kulkarni2015picture] follows a general strategy of defining a probabilistic with latent parameters, then using an inference algorithm to find the most appropriate set of latent parameters given the observations. Tieleman et al. [-@tieleman2014optimizing] moved beyond this two-stage pipeline by using a generic encoder network and a domain-specific decoder network to approximate a 2D rendering function. However, none of these approaches have been shown to automatically produce a semantically-interpretable graphics code and to learn a 3D rendering engine to reproduce images.

I present an approach, first described in [@kulkarni2015deep], which attempts to learn interpretable graphics codes for complex transformations such as out-of-plane rotations and lighting variations. Given a set of images, we use a hybrid encoder-decoder model to learn a representation that is disentangled with respect to various transformations such as object out-of-plane rotations and lighting variations. We employ a deep directed graphical model with many layers of convolution and de-convolution operators that is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [@kingma2013auto].

We propose a training procedure to encourage each group of neurons in the graphics code layer to distinctly represent a specific transformation. To learn a disentangled representation, we train using data where each mini-batch has a set of active and inactive transformations, but we do not provide target values as in supervised learning; the objective function remains reconstruction quality. For example, a nodding face would have the 3D elevation transformation active but its shape, texture and other transformations would be inactive. We exploit this type of training data to force chosen neurons in the graphics code layer to specifically represent active transformations, thereby automatically creating a disentangled representation. Given a single face image, our model can re-generate the input image with a different pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.
