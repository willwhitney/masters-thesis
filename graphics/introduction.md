# Disentanglement in Vision

## Introduction

Deep learning has led to remarkable breakthroughs in learning hierarchical representations from images. Models such as Convolutional Neural Networks (CNNs) [@lecun1995convolutional], Restricted Boltzmann Machines, [@hinton2006fast, @salakhutdinov2009deep], and Auto-encoders [@bengio2009learning, @vincent2010stacked] have been successfully applied to produce multiple layers of increasingly abstract visual representations. However, there is relatively little work on characterizing the optimal representation of the data. While  Cohen et al. [-@cohen2014learning] have considered this problem by proposing a theoretical framework to learn irreducible representations with both invariances and equivariances, coming up with the best representation for any given task is an open question.

Various work [@bengio2013representation, @cohen2014learning, @goodfellow2009measuring] has been done on the theory and practice of representation learning, and from this work a consistent set of desiderata for representations has emerged: invariance, interpretability, abstraction, and disentanglement. In particular, Bengio et al. [-@bengio2013representation] propose that a _disentangled_ representation is one for which changes in the encoded data are sparse over real-world transformations; that is, changes in only a few latents at a time should be able to represent sequences which are likely to happen in the real world.

The "vision as inverse graphics" paradigm suggests a representation for images which provides these features. Computer graphics consists of a function to go from compact descriptions of scenes (the _graphics code_) to images, and this graphics code is typically disentangled to allow for rendering scenes with fine-grained control over transformations such as object location, pose, lighting, texture, and shape. This encoding is designed to easily and interpretably represent sequences of real data so that common transformations may be compactly represented in software code; this criterion is conceptually identical to that of Bengio et al., and graphics codes conveniently align with the properties of an ideal representation.

Recent work in inverse graphics [@mansinghka2013approximate, @kulkarni2014inverse, @kulkarni2015picture] follows a general strategy of defining a probabilistic with latent parameters, then using an inference algorithm to find the most appropriate set of latent parameters given the observations. Recently, Tieleman et al. [-@tieleman2014optimizing] moved beyond this two-stage pipeline by using a generic encoder network and a domain-specific decoder network to approximate a 2D rendering function. However, none of these approaches have been shown to automatically produce a semantically-interpretable graphics code and to learn a 3D rendering engine to reproduce images.

In this paper, we present an approach which attempts to learn interpretable graphics codes for complex transformations such as out-of-plane rotations and lighting variations. Given a set of images, we use a hybrid encoder-decoder model to learn a representation that is disentangled with respect to various transformations such as object out-of-plane rotations and lighting variations. We employ a deep directed graphical model with many layers of convolution and de-convolution operators that is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [@kingma2013auto].

We propose a training procedure to encourage each group of neurons in the graphics code layer to distinctly represent a specific transformation. To learn a disentangled representation, we train using data where each mini-batch has a set of active and inactive transformations, but we do not provide target values as in supervised learning; the objective function remains reconstruction quality. For example, a nodding face would have the 3D elevation transformation active but its shape, texture and other transformations would be inactive. We exploit this type of training data to force chosen neurons in the graphics code layer to specifically represent active transformations, thereby automatically creating a disentangled representation. Given a single face image, our model can re-generate the input image with a different pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.
