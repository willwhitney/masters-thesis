# Discussion

## DC-IGN

We have shown that it is possible to train a deep convolutional inverse graphics network with a fairly disentangled, interpretable graphics code layer representation from static images. By utilizing a deep convolution and de-convolution architecture within a variational autoencoder formulation, our model can be trained end-to-end using back-propagation on the stochastic variational objective function [@kingma2013auto]. We proposed a training procedure to force the network to learn disentangled and interpretable representations. Using 3D face and chair analysis as a working example, we have demonstrated the invariant and equivariant characteristics of the learned representations.

Such a representation is powerful because it teases apart the true generating factors for images. Unlike a traditional deep representation, the representation generated by the DC-IGN separates the innate properties of an object from the results of its particular lighting and position. This brings us ever so slightly closer to a truly human-like understanding of 3D scenes, in which we use our knowledge of the structure of the world to correctly interpret the contributions to an image from depth, lighting, deformation, occlusion, and so much more. It is essential that our representations have this structure, as it allows incredible feats of imagination and prediction that current machine learning systems cannot even approach.

### Future work

To scale our approach to handle more complex scenes, it will likely be important to experiment with deeper architectures in order to handle large number of object categories within a single network architecture. It is also very appealing to design a spatio-temporal based convolutional architecture to utilize motion in order to handle complicated object transformations. Importance-weighted autoencoders [@burda2015importance] might be better able to capture more complex probabilistic structure in scenes. Furthermore, the current formulation of SGVB is restricted to continuous latent variables. However, real-world visual scenes contain unknown number of objects that move in and out of frame. Therefore, it might be necessary to extend this formulation to handle discrete distributions [@kulkarni2014variational] or extend the model to a recurrent setting. The decoder network in our model can also be replaced by a domain-specific decoder [@nair2008analysis] for fine-grained model-based inference.

In scenes with greater multimodal structure, perhaps in the future it may be possible to impose similar structure on the representation of a Deep Convolutional Generative Adversarial Network [@radford2015unsupervised]. These models seem to have the ability to capture much more complicated distributional structure than the simple Gaussian of the variational autoencoder.

## Controller-function networks

With the design of controller-function networks, I have provided a modern take on the mixture of experts model and shown that it is possible learn highly disentangled representations of computation. Furthermore, I have shown that at least in some cases, doing so imposes _no performance penalty_ on the system. These experiments have also demonstrated that disentangled representations of computation are much more resistant to the perennial problem of catastrophic forgetting, which has plagued neural methods since their inception.

### Future work

#### Multi-step variant {#sec:multistep}

![**Multistep CFN.** A variant of the design for using multiple timesteps (in this case, two) to calculate each output.](../figures/multistep_small.png){#fig:multistep}

One obvious question to consider about this model is, "What happens if the correct output function at a timestep is not computable with a linear combination of single-layer networks?" After all, there are functions computable by a polynomial-width network of depth `k` that require exponential width to compute with a network of depth `k-1` [@hastad1986almost].

To address this question, the system could be run for a predefined number of steps between outputs. That is,

1. Feed the system the input for "real-world" time `t` and save the output
2. Repeat `k` times:
	a. Feed the system its own most recent output
4. Take the `kth` output of the system as its answer for time `t`
5. Repeat from 1. for time `t+1`

This amounts to making the network deeper, in that more layers of computation and nonlinearity lie between the input and the output. This gives it the same computational power of a `k`-depth model.

While correctly learning the full latent factorization of data from deeply-composed observations may be quite difficult, we can persist in the theme of continuation learning and train such a system with a curriculum, learning first to do one-step computations, then two, three, and so forth until we have quite a complex model while still preserving the disentangled nature of our representation. Various experiments done by [@bengio2009curriculum, @zaremba2014learning, @reed2015neural], among many others, have shown using curricula with increasing complexity to be extremely effective.

#### Complex metadata

While in the experiments described here the metadata given to the controller is quite simple, the possibilities for the future are rich. In the most straightforward case, the controller could read in source code for a program one character at a time, then predict a sequence of activations to compose the program it has read out of primitive functions in the manner described in [@Sec:multistep].

Even richer representations are also possible. The controller might for example look at two images, then predict a series of structured transformations to turn one into the other.

#### Non-differentiable functions

One unexplored capability of the controller-function network is that the controller can produce weightings over "functions" or actions which are not differentiable, while still being trained itself by backpropagation. This is due to the structure of the output function:

$$Out = \sum_{i=1}^F C(x)_ {i} f_i(x))$$

$$\frac{\delta Out}{\delta C_i} = f_i(x)$$

The gradient of the controller does not depend on the gradient of the functions. As a result, the functions that the controller uses could be non-differentiable, ordinary handwritten programming functions. They could even be _actions_ in the world, as long as those actions have a continuous representation, e.g. `tighten_left_calf(force)`.

This convenient fact also has computational implications; if one were to construct a hierarchy of CFNs, such that a higher-level CFN provides weights on the functions computed by lower-level ones, all gradients could be computed simultaneously and independently. This provides an interesting avenue for future research in hierarchical computation.

## Unification

Perhaps the most exciting direction for future work is the bringing together of these two different techniques in order to build a completely unsupervised system for inferring the factors of variation in the world and using that structure to represent and predict videos.

Instead of using our knowledge of the active transformations over a short video, we could train a controller network to _weight_ many different transformations for each frame. Then, using the same continuation methods described in this paper, we could sharpen those weightings gradually over the course of training, resulting in a representation of real video which is extremely sparse. Such a representation might be able to capture the true generative structure of simple scenes without any supervision at all, learning dimensions of variation which are most common in the real world.

I look forward to continuing my work on building systems which can understand the factors of variation in the world, and I hope that this thesis provides some small help and inspiration to others in the field.
